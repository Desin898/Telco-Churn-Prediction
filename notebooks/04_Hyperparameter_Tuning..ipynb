{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter tuning for Decision Tree",
   "id": "8d2b095569460bad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:18:13.754601Z",
     "start_time": "2025-12-09T16:18:11.147692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('../data/telco_churn_processed.csv')\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define the \"Grid\" of settings to test\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# 3. Setup the Grid Search\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "# 4. Run the Search\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. The Results\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\n✅ Best Parameters Found: {best_params}\")\n",
    "print(f\"✅ Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# 6. Evaluate the \"Optimized\" Model on Test Data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_acc = accuracy_score(y_test, best_model.predict(X_test))\n",
    "print(f\"Test Set Accuracy of Optimized Tree: {test_acc:.4f}\")"
   ],
   "id": "b9f473ccea40d53b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "✅ Best Parameters Found: {'criterion': 'entropy', 'max_depth': 7, 'min_samples_split': 2}\n",
      "✅ Best Cross-Validation Accuracy: 0.7931\n",
      "Test Set Accuracy of Optimized Tree: 0.7765\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Neural Network Hyperparameter Tuning\n",
    "Unlike Decision Trees, where `GridSearch` is computationally inexpensive, Neural Networks require a **Manual Architecture Search**. We conduct a controlled experiment to determine if increasing model complexity improves performance.\n",
    "\n",
    "### **Experimental Setup**\n",
    "* **Model A (Baseline):** The architecture defined in Notebook 03 (Input $\\to$ 16 Neurons $\\to$ 8 Neurons $\\to$ Output). Optimized with `Adam`.\n",
    "* **Model B (Experiment):** A deeper, wider architecture (Input $\\to$ 64 Neurons $\\to$ Dropout 20% $\\to$ 32 Neurons $\\to$ Output). Optimized with `RMSprop`.\n",
    "\n",
    "### **Hypothesis**\n",
    "A larger model with Dropout regularization (Model B) should capture more complex non-linear patterns and generalize better than the simpler Model A."
   ],
   "id": "fe3e32af427f928e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:25:09.310914Z",
     "start_time": "2025-12-09T16:24:56.302790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "# 1. Define the Baseline Score\n",
    "accuracy_baseline = 0.7922\n",
    "\n",
    "# 2. Define Model B\n",
    "model_b = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),   # More neurons (was 16)\n",
    "    Dropout(0.2),                   # Drop 20% of neurons to prevent overfitting\n",
    "    Dense(32, activation='relu'),   # More neurons (was 8)\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 3. Compile with a different optimizer\n",
    "model_b.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train Model B\n",
    "print(\"Training Neural Network Experiment B...\")\n",
    "history_b = model_b.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# 5. Evaluate\n",
    "loss_b, acc_b = model_b.evaluate(X_test, y_test)\n",
    "print(f\"\\nModel A (Baseline) Accuracy: {accuracy_baseline:.4f}\")\n",
    "print(f\"Model B (Tuned) Accuracy:    {acc_b:.4f}\")\n",
    "\n",
    "# Conclusion Logic\n",
    "print(\"-\" * 30)\n",
    "if acc_b > accuracy_baseline:\n",
    "    print(\"OBSERVATION: The complex architecture (Model B) improved accuracy.\")\n",
    "    print(\"RECOMMENDATION: Adopt Model B for deployment.\")\n",
    "else:\n",
    "    print(\"OBSERVATION: Increasing complexity (Model B) resulted in similar or lower accuracy.\")\n",
    "    print(\"CONCLUSION: The simpler Model A is more robust. We recommend proceeding with the Baseline model to reduce computational cost.\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "4ec52a0611f1dfcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network Experiment B...\n",
      "\u001B[1m44/44\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.7922 - loss: 0.4290 \n",
      "\n",
      "Model A (Baseline) Accuracy: 0.7922\n",
      "Model B (Tuned) Accuracy:    0.7922\n",
      "------------------------------\n",
      "OBSERVATION: Increasing complexity (Model B) resulted in similar or lower accuracy.\n",
      "CONCLUSION: The simpler Model A is more robust. We recommend proceeding with the Baseline model to reduce computational cost.\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
